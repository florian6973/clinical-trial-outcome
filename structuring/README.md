# Outcome structuring

Folder structure:
- clustering: code to classify outcome objects into large categories
- ner: code to breakdown the outcome titles and structure them according to the data model 
- snomed: code to evaluate the direct outcome SNOMED mapping, with annotations

Outputs are not included in the github repository.

## Clustering (WIP)

This folder contains the code to explore the results of the structuring output 
and to classify outcome objects into large categories:
- `preprocess-object` cleans the output from the BERT model
- `embeddings` computes the embeddings of the objects
- `clustering` clusters these embeddings to see if some categories can be isolated (but it is not clear)
- `classification`, `classification-full` runs Llama to classify the outcome objects to the categories from COMET
- `diff_res` allows to compare the results with different methods
- `counts` computes some statistics
- `test-set` samples some elements to check the accuracy

## NER (Named Entity Recognition)

The annotated dataset can be found under the `data` subfolder. There are the 250 annotations `manual-ann-ner-fp` for the structuring, and the 100 annotations `group-ann-fp` for the normalization.

The `llm` folder contains the code to run the models (BERT-like and LLMs):
- `annotations` preprocesses the data and handles the conversion to the BIO format
- `convert_chatgpt` contains the code to postprocess GPT-4 output to convert it to our format.
- `dspy_run` and `dspy_test` contain the code to test the the DSPY framework to optimize the prompt (https://dspy.ai/tutorials/entity_extraction/). However it did not lead to any significant improvement.
- `llama_lora_adapter` contains the code to finetune Llama for our specific Information Extraction task (WIP)
- `llm` contains the utility functions for running LLMs
- `ner_bert` contains the code specific to BERT-like model training
- `ner_llm` contains the code specific to LLMs
- `read_outcomes` contains the code to read outcomes
- `train` contains the main code to launch training, evaluation or inference of the different models. Results will be saved in `outputs`

The `stats` folder contains the code to compute the metrics and plot the graphs:
- `agreement` computes the Cohen's kappa score
- `compute_complexity` runs a complexity analysis of the outcomes
- `evaluate_fuzzy` runs the evaluation and compute NLD and Differences.
- `evaluate` runs the evaluation with exact string matching.`
- `extracts_stats` computes the most frequent labels per field
- `visualization` contains the code for trend analysis

## SNOMED

The annotated SNOMED dataset has been generated by taking the centroids of 100 k-means clusters.

Workflow:
    - read_csv.py to convert csv to numpy memmap (generated in the 'normalization' folder)
    - cluster_sampling.py to select samples
    - info_ann.py to display and manually annotate the examples
    - compute_score.py to show stats

